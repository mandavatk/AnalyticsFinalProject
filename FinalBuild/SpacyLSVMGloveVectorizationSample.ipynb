{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analytics Experience - Group Project - MSCI:6040 \n",
    "\n",
    "By Group 2 (Chris Wagner, Palden Williams, Tarun Mandava  - Data Scientists) \n",
    "- September, 2020                                                             \n",
    "\n",
    "Objective - This project is an exercise in using natural language processing to determine if social media posts meet a pre-specified criterion.  In this case, the test is to determine which social media posts truly reflect information tied to a natural disaster.  For this exercise, we are working with a test dataset of Twitter posts that have been manually reviewed from Kraggle.\n",
    "\n",
    "Data source:  Real or Not?  NLP with Disaster Tweets dataset from Kaggle\n",
    "\n",
    "https://www.kaggle.com/c/nlp-getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Library generated Vectorization with SVC Model\n",
    "\n",
    "This sample is looking at using the Spacy Library with its precompiled vectors to generate a Glove-like vectorization for our model.  Again, we'll use the Support Vector Classification out of the SVM library in SKLearn for the predictive modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Acknowledgements/Sources\n",
    "\n",
    "https://faculty.ai/blog/glove/<br>\n",
    "https://nlp.stanford.edu/projects/glove/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mplt\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt                     \n",
    "import numpy as np           \n",
    "import plotly.offline as plyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "sn.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the Kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_data = pd.read_csv(\"Data/OriginalData/sample_submission.csv\")\n",
    "test_data = pd.read_csv(\"Data/OriginalData/test.csv\")\n",
    "train_data = pd.read_csv(\"Data/OriginalData/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove any html characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def  remove_html(df, text):\n",
    "    df[text] = df[text].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    return df\n",
    "\n",
    "test_clean = remove_html(test_data, \"text\")\n",
    "train_clean = remove_html(train_data, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove any Emojii's from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "test_clean['text']=test_clean['text'].apply(lambda x: remove_emoji(x))\n",
    "train_clean['text']=train_clean['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "The following functions will be used to remove punctuation and stopwords, set common lemmatizations, and force the data to lowercase from the incomming text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "# Clean text before feeding it to model\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, puncuation and reducing all characters to lowercase \n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    for doc in tqdm(docs):\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        #remove stopwords and punctuations \n",
    "        tokens = [tok for tok in tokens if tok not in stop and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7613/7613 [00:18<00:00, 408.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3263/3263 [00:07<00:00, 427.31it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data['token'] = cleanup_text(train_data['text'],logging=False)\n",
    "test_data['token'] = cleanup_text(test_data['text'],logging=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will create a class that will inherit from the sklearn BaseEstimator and TransformerMixin libraries.  It overrides the fit and transform functions to utilize the internal nlp object for creating the model vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code provided by faculty.ai at https://faculty.ai/blog/glove/\n",
    "\n",
    "#Note:  This code is included in the GloveVec.py file in the project folder\n",
    "\n",
    "#import numpy as np\n",
    "#import spacy\n",
    "#from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#class GloveVectorizer(BaseEstimator, TransformerMixin):\n",
    "#    def __init__(self, model_name=\"en_core_web_md\"):\n",
    "#        self._nlp = spacy.load(model_name)\n",
    "#\n",
    "#    def fit(self, X, y=None):\n",
    "#        return self\n",
    "#\n",
    "#    def transform(self, X):\n",
    "#        return np.concatenate(\n",
    "#            [self._nlp(doc).vector.reshape(1, -1) for doc in X]\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the GloveVectorizer from the py file to see if this allows the flask app to run\n",
    "from GloveVec import GloveVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to identify the contents, we created specific variables for the X and Y training values, and an X_Test used to pull the return values to return to Kaggle for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train_data[\"token\"]\n",
    "y=train_data[\"target\"]\n",
    "X_Test = test_data[\"token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model the training set of Twitter data\n",
    "\n",
    "A pipeline was used to create an instance of the GloveVectorizer class as well as a new SVC model.  After the model is created, it is fit with the X and Y values from the training set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\base.py:197: FutureWarning:\n",
      "\n",
      "From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('glovevectorizer', GloveVectorizer(model_name=None)),\n",
       "                ('svc',\n",
       "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = GloveVectorizer()\n",
    "svm = svm.SVC()\n",
    "\n",
    "pline = make_pipeline(g,svm)\n",
    "\n",
    "pline.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has been created, let's review the score with the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pline.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8534086431104689"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll pull the sample submission dataset, score it against our model, and retrun the value to Kaggle for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('Data/OriginalData/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.target = pline.predict(X_Test)\n",
    "sub.to_csv(\"spacy_svm_glove_submission_clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub.target.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Package the model into a pickle file for later use\n",
    "\n",
    "The model will be stored in a pickle file so it can be called later from the flask application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(pline, 'svm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test against live tweets\n",
    "\n",
    "For a further example, we pulled a sample of 100 tweets from Twitter with the keyword \"wildfires\".  Let's apply that to the model to see how it performed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"recent_tweets.csv\")\n",
    "\n",
    "#store the original tweet for comparison\n",
    "tweets['OriginalText'] = tweets['Text']\n",
    "\n",
    "tweets = remove_html(tweets,\"Text\")\n",
    "tweets['Text']=tweets['Text'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "tweets['token'] = cleanup_text(tweets['Text'],logging=False)\n",
    "\n",
    "t_test = tweets[\"token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the disaster tweets\n",
    "\n",
    "tweets[\"Target\"] = pline.predict(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the results of disaster vs. non-disaster\n",
    "\n",
    "tweets.Target.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the results to a csv to review the final results\n",
    "\n",
    "tweets.to_csv(\"recent_tweets_scored.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
