{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analytics Experience - Group Project - MSCI:6040 \n",
    "\n",
    "By Group 2 (Chris Wagner, Palden Williams, Tarun Mandava  - Data Scientists) \n",
    "- September, 2020                                                             \n",
    "\n",
    "### Modeling SVC with Count Vectorizer text classification\n",
    "\n",
    "This sample will use the Count Vectorizer for organizing the tweet text into common tokens.  This will then be applied to the SVC model we determined in the model evaluation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies and the libraries to import\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mplt\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt                     \n",
    "import numpy as np           \n",
    "import plotly.offline as plyo\n",
    "#import cufflinks as cf\n",
    "#plyo.init_notebook_mode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Further library imports\n",
    "\n",
    "import random\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "from tqdm import tqdm\n",
    "stop = stopwords.words('english')\n",
    "sn.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the sample data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Kaggler data set\n",
    "\n",
    "sub = pd.read_csv(\"Data/OriginalData/sample_submission.csv\")\n",
    "test_data = pd.read_csv(\"Data/OriginalData/test.csv\")\n",
    "train_data = pd.read_csv(\"Data/OriginalData/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove any html characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def  remove_html(df, text):\n",
    "    df[text] = df[text].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    return df\n",
    "\n",
    "test_clean = remove_html(test_data, \"text\")\n",
    "train_clean = remove_html(train_data, \"text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove any Emojii's from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "test_clean['text']=test_clean['text'].apply(lambda x: remove_emoji(x))\n",
    "train_clean['text']=train_clean['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Break up the text into common tokens based on the spacy library\n",
    "\n",
    "This function will use the en_core_web_sm library to break up the tweet text into common components.  It will also remove any stopwords, punctuation, and exclude any pronouns.\n",
    "\n",
    "The Lemma component will group words to their common root such as run, ran, running will all be associated with the word run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "# Clean text before feeding it to model\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, puncuation and reducing all characters to lowercase \n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    for doc in tqdm(docs):\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        #remove stopwords and punctuations \n",
    "        tokens = [tok for tok in tokens if tok not in stop and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cleanup text function will be used to create a list of valid tokens per tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7613/7613 [00:17<00:00, 423.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3263/3263 [00:07<00:00, 438.75it/s]\n"
     ]
    }
   ],
   "source": [
    "train_clean['token'] = cleanup_text(train_clean['text'],logging=False)\n",
    "test_clean['token'] = cleanup_text(test_clean['text'],logging=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>get send photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1   4     NaN      NaN              Forest fire near La Ronge Sask Canada   \n",
       "2   5     NaN      NaN  All residents asked to shelter in place are be...   \n",
       "3   6     NaN      NaN  13000 people receive wildfires evacuation orde...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "   target                                              token  \n",
       "0       1           deed reason earthquake may allah forgive  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  resident ask shelter place notify officer evac...  \n",
       "3       1  13000 people receive wildfire evacuation order...  \n",
       "4       1  get send photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "The count vectorizer will be used to create a corpus of all of the words in the training set.  This will then be used to apply the words in any test set against the original train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector for both the train and test sets\n",
    "\n",
    "train_vectors = cv.fit_transform(train_clean[\"token\"])\n",
    "test_vectors = cv.transform(test_clean[\"token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14025)\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_vectors[0].todense().shape)\n",
    "print(train_vectors[0].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the vectors to a SVC model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the SVC model\n",
    "sv= svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72619829, 0.65659882, 0.67235719, 0.6977661 , 0.77595269])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score the svc model against the vectors generated by the count vectorizer\n",
    "scores = cross_val_score(sv, train_vectors, train_clean[\"target\"], cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores look acceptable.  We'll now train the model based on the train vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv.fit(train_vectors, train_clean[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now predict the test set for returning to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.target = sv.predict(test_vectors)\n",
    "sub.to_csv(\"countVect_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAALKElEQVR4nO3dT4jc93nH8fdHK1qbHLr4j5rYa1tpsVriuIkdmxjqzaWUHEKgDSFExc6hJyUG42PwIW4PBVF8EKkdpB5aJzi4GBxCIIUYH4olnEJwIjetURQSra11QmJb2UAPNlR+etif2tF4pd2dXc9o93m/YNjd7zN/voJl3/ub38wqVYUkqa89s96AJGm2DIEkNWcIJKk5QyBJzRkCSWpu76w3sFlJfhe4G/glcH7G25GknWIO+ADwg6p6e3Sw40LAagSOz3oTkrRDLQInRhd2Ygh+CXD8+HEWFhZmvRdJ2hGWl5dZXFyE4WfoqJ0YgvMACwsL7N+/f8ZbkaQd511PqXuyWJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWpuJ76PYMfY/+XvznoLu8bS4U/NegvSruURgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWpu3RAkuTbJvyb5SZL/SPKtJNcPs3uSvJTkdJJnk+wbud1EM0nSdG3kiKCAv6+qP6qqPwF+BhxOEuBJ4IGqOgA8DxwGmHQmSZq+dUNQVeeq6t9Glv4duAW4C3irqk4M60eBzw2fTzqTJE3Z3s1cOcke4IvAd4CbgVcuzKrqjSR7klwz6ayqzo093jwwP7aNhc3sWZJ0eZs9WfwPwH8Dj70He1nLQ8CZscvxKT22JLWw4SOCJI8CtwKfrqp3krzK6lNEF+bXAVVV5yadrfGwR4AnxtYWMAaStG02dESQ5O+AjwF/UVVvD8svAlcnuXf4+hDw9BZnF6mqlapaGr0Ayxv7p0mSNmLdI4IktwEPA6eBF1Zf9MOZqvrLJPcDx5JcBSwB9wEMRwybnkmSpm/dEFTVfwG5xOwF4PbtnEmSpst3FktSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDW3oRAkeTTJmSSV5MMj60tJTiU5OVw+OTK7J8lLSU4neTbJvo3MJEnTtdEjgm8DnwBeWWP22ar66HD5HkCSAE8CD1TVAeB54PB6M0nS9O3dyJWq6gTA6s/wDbkLeOvC7YCjwBLw1+vMLpJkHpgfW17Y6CYkSevbUAjW8c3ht/wTwMNVtQLczMjRQ1W9kWRPkmsuN6uqc2P3/RDwyDbsUZJ0CVs9WbxYVR8B7gYCPLb1LV3kCPDBscviNj+GJLW2pSOCqjo7fHw7ydeA7wyjV4FbLlwvyXWrV6tzSS45W+P+V4CV0bVNPD0lSdqAiY8Ikrwvye8Nnwf4PHByGL8IXJ3k3uHrQ8DTG5hJkqZsQ0cESb4KfAZ4P/BckjeBTwPPJJkD5oCXgS8BVNU7Se4HjiW5itWTwfetN5MkTd9GXzX0IPDgGqM7LnObF4DbNzuTJE2X7yyWpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc2tG4IkjyY5k6SSfHhk/UCS7yc5PXy8daszSdL0beSI4NvAJ4BXxtaPAo9X1QHgceDYNswkSVO2bgiq6kRVnR1dS7IPuBN4alh6CrgzyfWTzrb+T5EkTWLvhLe7CXitqs4DVNX5JL8Y1jPh7PXxB0kyD8yPLS9MuGdJ0homDcG0PAQ8MutNSLvN/i9/d9Zb2FWWDn9q1lvYkklDcBa4Mcnc8Fv9HHDDsJ4JZ2s5AjwxtrYAHJ9w35KkMRO9fLSqfg2cBA4OSweBH1XV65POLvE4K1W1NHoBlifZsyRpbeseEST5KvAZ4P3Ac0nerKrbgEPA15N8BfgN8IWRm006kyRN2bohqKoHgQfXWD8FfPwSt5loJkmaPt9ZLEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1NyWQ5BkKcmpJCeHyyeH9XuSvJTkdJJnk+wbuc0lZ5Kk6dquI4LPVtVHh8v3kgR4Enigqg4AzwOHAS43kyRN33v11NBdwFtVdWL4+ijwuQ3MJElTtneb7uebw2/6J4CHgZuBVy4Mq+qNJHuSXHO5WVWdG73TJPPA/NhjLWzTniVJbM8RwWJVfQS4Gwjw2Dbc5wUPAWfGLse38f4lqb0th6Cqzg4f3wa+Bvwp8Cpwy4XrJLlu9Sp1bp3ZuCPAB8cui1vdsyTp/23pqaEk7wP2VtVvh6eGPg+cBF4Erk5y73Au4BDw9HCzy80uUlUrwMrYY25ly5KkMVs9R/D7wDNJ5oA54GXgS1X1TpL7gWNJrgKWgPsALjeTJE3flkJQVT8H7rjE7AXg9s3OJEnT5TuLJak5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmjMEktScIZCk5gyBJDVnCCSpOUMgSc0ZAklqzhBIUnOGQJKaMwSS1JwhkKTmDIEkNWcIJKk5QyBJzRkCSWrOEEhSc4ZAkpozBJLUnCGQpOYMgSQ1ZwgkqTlDIEnNGQJJas4QSFJzhkCSmptZCJIcSPL9JKeHj7fOai+S1NksjwiOAo9X1QHgceDYDPciSW3tncWDJtkH3An8+bD0FPBYkuur6vWR680D82M3vwVgeXl5Glvdkv/57a9mvYVdY2lpadZb2FX83txeO+H7c+Rn5tz4LFU13d0AST4GfKOqbhtZexm4r6p+OLL2N8AjU9+gJO1ei1V1YnRhJkcEm3AEeGJs7XeAPwB+Cpyf9oZ2mQXgOLAIXPmHWOrG78/tNQd8APjB+GBWITgL3JhkrqrOJ5kDbhjW/09VrQAra9z+9BT2uOslufDpclUtzXAr0rv4/fme+NlaizM5WVxVvwZOAgeHpYPAj0bPD0iSpmOWTw0dAr6e5CvAb4AvzHAvktTWzEJQVaeAj8/q8SVJq3xncW8rwN+y9nkYadb8/pySmbx8VJJ05fCIQJKaMwSS1JwhkKTmrvR3Fus9kORa4Kbhy7NV9eYs9yNptjxZ3EiSPwT+kdU/+PeLYfkG4IfAoar66az2Jml2fGqol28A/wRcW1W3DX/071rgn4eZdEVK8uNZ72E384igkSSnquqPNzuTpiHJhy4zfq6qbpjaZprxHEEv55IcBP6lht8AsvqXvf4K37Sj2ftPYAnIGrPrpruVXjwiaGT470CPAncArw3LN7L6BwC/WFU/mdXepCQ/Z/Vv5b+2xuxsVd20xs20DTwiaGQ4GfxnSa7n4lcN+VdfdSV4htX/gfBdIQC+NeW9tOIRgSQ156uGJKk5QyBJzRkCSWrOEEhSc4ZAkpr7X7e2UpqKVFEJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub.target.value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
